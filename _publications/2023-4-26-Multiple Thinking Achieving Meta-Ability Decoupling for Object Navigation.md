---
title: "Multiple Thinking Achieving Meta-Ability Decoupling for Object Navigation"
collection: publications
permalink: /publication/2023-4-26-Multiple Thinking Achieving Meta-Ability Decoupling for Object Navigation
excerpt: 'This article proposes a new paradigm of meta-capability decoupling for Embodied AI tasks, which fully improves the interpretability, transferability and scalability of the model.'
date: 2023-4-06
venue: 'ICML'
paperurl: 'http://academicpages.github.io/files/MAD.pdf'
citation: 'Dang, Ronghao, et al. "Multiple thinking achieving meta-ability decoupling for object navigation." arXiv preprint arXiv:2302.01520 (2023).'
---
Abstract: We propose a meta-ability decoupling (MAD) paradigm, which brings together various object navigation methods in an architecture system, allowing them to mutually enhance each other and evolve together. Based on the MAD paradigm, we
design a multiple thinking (MT) model that leverages distinct thinking to abstract various metaabilities. Our method decouples meta-abilities from three aspects: input, encoding, and reward while employing the multiple thinking collaboration (MTC) module to promote mutual cooperation between thinking. MAD introduces a novel qualitative and quantitative interpretability system for object navigation. Through extensive experiments on AI2-Thor and RoboTHOR, we
demonstrate that our method outperforms state-of-the-art (SOTA) methods on both typical and zero-shot object navigation tasks.

[Download paper here](http://academicpages.github.io/files/MAD.pdf)
